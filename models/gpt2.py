import json
import regex as re
from typing import Dict, List, Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F


class GPT2BPE:

    def __init__(self, byte_to_unicode, unicode_to_byte, base_toks, regex_str):
        self.byte_to_unicode = byte_to_unicode
        self.unicode_to_byte = unicode_to_byte

        self.tok2id: Dict[str, int] = {}
        self.id2tok: Dict[int, str] = {}
        for ch in base_toks:
            i = len(self.tok2id)
            self.tok2id[ch] = i
            self.id2tok[i] = ch

        self.regex_pat = re.compile(regex_str) if regex_str is not None else None
        self.bpe_ranks: Dict[Tuple[str, str], int] = {}

        self.sos_tok = None
        self.eos_tok = None
        # they should init by training process (or preloading) and be part of tok2id, 

    def _chunks(self, text: str) -> List[str]:
        """
        normal str -> chunkify.
        """
        if self.regex_pat is None:
            return [text]
        return [m.group(0) for m in self.regex_pat.finditer(text)]

    def _visible(self, text: str) -> str:
        """
        raw text -> utf-8 bytes -> visible stand-ins.
        """
        return "".join(self.byte_to_unicode[b] for b in text.encode("utf-8"))

    def train(self, text, vocab_size, verbose=False):
        """
        Uses the configured pre-splitter:
        regex_pat is None: stream mode (one big chunk; merges can cross anything)
        regex_pat: regex-chunked (merges confined within chunks; GPT-style)
        Training mutates tok2id/id2tok and bpe_ranks.
        """

        assert vocab_size >= len(
            self.tok2id
        ), "vocab_size must be >= 256 (base alphabet + any pre-added specials)."

        new_merges = vocab_size - len(self.tok2id)

        # precreate a freq dictonary of chunks so instead of iterating over all chunks we iterate on this
        chunks = self._chunks(text)
        from collections import Counter
        chunk_freq = Counter()
        for ch in chunks:
            vis = self._visible(ch)
            chunk_freq[tuple(vis)] += 1

        # iteratively pick the most frequent adjacent pair, assign next rank, update words
        for step in range(new_merges):
            pair_freq = {}
            for syms, f in chunk_freq.items(): # (tuple, frequency)
                if len(syms) < 2:
                    continue
                for i in range(len(syms) - 1):
                    p = (syms[i], syms[i + 1])
                    pair_freq[p] = pair_freq.get(p, 0) + f
            if not pair_freq:
                print(f"[train] stop: no pairs at step {step}")
                break

            best = max(pair_freq.items(), key=lambda kv: kv[1])[0]  # (a,b) to merge
            rank = len(self.bpe_ranks)
            self.bpe_ranks[best] = rank

            merged_tok = best[0] + best[1]
            
            assert merged_tok not in self.tok2id
            nid = len(self.tok2id)
            self.tok2id[merged_tok] = nid
            self.id2tok[nid] = merged_tok

            a, b = best

            def merge_once(syms):
                """Replace every (a,b) with 'a+b' inside one tuple of unicodes."""
                out: List[str] = []
                i = 0
                while i < len(syms):
                    if i + 1 < len(syms) and syms[i] == a and syms[i + 1] == b:
                        out.append(merged_tok)
                        i += 2
                    else:
                        out.append(syms[i])
                        i += 1
                return tuple(out)

            new_freq = {}
            for syms, f in chunk_freq.items():
                ms = merge_once(syms)
                new_freq[ms] = new_freq.get(ms, 0) + f

            chunk_freq = new_freq

            if verbose:
                print(f"merge {step+1}/{new_merges}: {best} -> '{merged_tok}' | occurence: {pair_freq[best]} | rank={rank}")

        # specials:
        nid = len(self.tok2id)
        self.tok2id['<sos>'] = nid
        self.id2tok[nid] = '<sos>'
        self.sos_tok = '<sos>'

        nid = len(self.tok2id)
        self.tok2id['<eos>'] = nid
        self.id2tok[nid] = '<eos>'
        self.eos_tok = '<eos>'

    def _bpe(self, vis_chunk: str) -> List[str]:
        """
        Apply BPE merges greedily (by rank) within ONE visible chunk.
        Returns a list of merged token strings (each 1+ stand-in chars).
        """
        syms: List[str] = list(vis_chunk)
        while True:
            ps = {(syms[i], syms[i + 1]) for i in range(len(syms) - 1)}
            if not ps:
                break
            # choose the present pair with the smallest rank number
            best = min(ps, key=lambda p: self.bpe_ranks.get(p, float("inf")))
            # if best comes from inf then need to check if such merge is possible
            if best not in self.bpe_ranks:
                break
            a, b = best
            # merge all (a,b) occurrences in a single pass
            out: List[str] = []
            i = 0
            while i < len(syms):
                if i + 1 < len(syms) and syms[i] == a and syms[i + 1] == b:
                    out.append(a + b)
                    i += 2
                else:
                    out.append(syms[i])
                    i += 1
            syms = out
            if len(syms) == 1:
                break
        return syms

    def encode(self, text: str, add_specials=False, verbose=False) -> List[int]:
        """
        Convert text: str -> token IDs: List
        """

        ids: List[int] = []

        for i, chunk in enumerate(self._chunks(text)):
            vis = self._visible(chunk)
            toks = self._bpe(vis)
            tok_ids = [self.tok2id[tok] for tok in toks]
            if verbose:
                print(f"chunk {i}: {chunk!r} | visible: {vis} | toks: {toks} | ids: {tok_ids}")
            ids.extend(tok_ids)

        if add_specials:
            if self.sos_tok:
                ids = [self.tok2id[self.sos_tok]] + ids
            if self.eos_tok:
                ids.append(self.tok2id[self.eos_tok])

        return ids

    def decode(self, ids) -> str:
        """
        Convert token IDs: List -> text: str. Fully lossless (reverses the byte map).
        """
        s: str = "".join(self.id2tok[i] for i in ids)
        return bytes(self.unicode_to_byte[ch] for ch in s).decode("utf-8")

    @classmethod
    def gpt2(cls, pretrained=False):
        
        # nice bytes kept as their own codepoints
        bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256))
        cs = bs[:]  # copy
        # remaining bytes remapped to U+0100, U+0101, ...
        n = 0
        for b in range(256):
            if b not in bs:
                bs.append(b)
                cs.append(256 + n)
                n += 1
        enc = {b: chr(c) for b, c in zip(bs, cs)}
        dec = {v: k for k, v in enc.items()}
        init_standins = [chr(c) for c in cs]

        init_bpe = cls(enc, dec, init_standins, (r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""))

        if not pretrained:
            return init_bpe
        
        # instead of training load the weights?
                
        from huggingface_hub import hf_hub_download
        vocab_path  = hf_hub_download(repo_id="openai-community/gpt2", filename="vocab.json")
        merges_path = hf_hub_download(repo_id="openai-community/gpt2", filename="merges.txt")

        with open(vocab_path, "r", encoding="utf-8") as f:
            enc = json.load(f)

        init_bpe.tok2id = {k: int(v) for k, v in enc.items()}
        init_bpe.id2tok = {int(v): k for k, v in enc.items()}

        with open(merges_path, "r", encoding="utf-8") as f:
            merges = [
                tuple(line.strip().split())
                for line in f
                if line and not line.startswith("#")
            ]

        init_bpe.bpe_ranks = {pair: i for i, pair in enumerate(merges)}
        init_bpe.eos_tok = '<|endoftext|>'
        return init_bpe
    

class GPT2Embedding(nn.Module):
    def __init__(self, vocab_size, max_length, embed_dim, dropout):
        super().__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.embed_dim = embed_dim

        self.tok_embed = nn.Embedding(self.vocab_size, self.embed_dim)
        self.pos_embed = nn.Embedding(self.max_length, self.embed_dim)
        self.dropout = nn.Dropout(dropout)

    @property
    def weight(self): # convenience alias
        return self.tok_embed.weight

    def forward(self, x, x_pos):
        # [B, SEQ_LENGTH]
        x = self.tok_embed(x) + self.pos_embed(x_pos) # (B, T, D) + (1, T, D): (B, T, D)
        # [B, SEQ_LENGTH, EMBED_DIM]
        return self.dropout(x)
        # Note: Essentially a table lookup of id to dense representation. Hence a huge matrix

class GPT2LMHead(nn.Module):
    def __init__(self, embed_dim, embedding):
        super().__init__()
        self.ln = nn.LayerNorm(embed_dim)
        self.embedding = embedding

    def forward(self, x):
        # [B, SEQ_LENGTH, EMBED_DIM]
        w = self.embedding.weight
        x = self.ln(x)
        out = F.linear(x, w)
        return out
        # [B, SEQ_LENGTH, VOCAB_SIZE]
        
class GPT2MHAttention(nn.Module):
    def __init__(self, max_length, embed_dim, num_heads, dropout):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.max_length = max_length
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = (embed_dim // num_heads)

        # fused qkv projection:
        self.qkv_w = nn.Linear(self.embed_dim, 3*self.embed_dim)
        self.proj_w = nn.Linear(self.embed_dim, self.embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, past_kv=None, use_cache=False):
        """
        x: [B, T_q, C], T_q = 1 for inference (usually)
        past_kv: (K_cache, V_cache) where K_cache, V_cache: [B, H, T_past, Dh]
        use_cache: if True, return (y, (K_total, V_total)); else return y
        """

        B, T_q, _ = x.size()
        x = self.qkv_w(x)
        qx, kx, vx = x.split(self.embed_dim, dim=2)

        # each [B, H, T_q, Hd]:
        qx = qx.view(B, T_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        kx = kx.view(B, T_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        vx = vx.view(B, T_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)

        mask = torch.tril(torch.ones(self.max_length, self.max_length, dtype=torch.bool)).view(1, 1, self.max_length, self.max_length).to(x.device)

        if past_kv is not None:
            kp, vp = past_kv                                                # [B, H, T_past, Dh]
            T_past = kp.size(2)
            Kx = torch.cat([kp, kx], dim=2)                                 # [B, H, T_past + T_q, Dh]
            Vx = torch.cat([vp, vx], dim=2)                                 # [B, H, T_past + T_q, Dh]
            attn_mask = mask[:, :, T_past:T_past+T_q, :T_past+T_q]     # [1, 1, T_q, T_past + T_q]
        else:
            Kx, Vx = kx, vx
            T_past = 0
            attn_mask = mask[:, :, :T_q, :T_q]                         # [1, 1, T_q, T_q]

        att_w = torch.einsum('bhqd,bhkd->bhqk', [qx, Kx]) / (self.head_dim ** 0.5)
        # [B, H, T_q, T_past + T_q]
        att_w = att_w.masked_fill(~attn_mask, torch.finfo(att_w.dtype).min)
        # [B, H, T_q, T_past + T_q]

        att_w = F.softmax(att_w, dim=-1)
        att_w = self.dropout(att_w)
        # [B, H, T_q, T_past + T_q]
        # droppin out probs why? (attn dropout)

        out = torch.einsum('bhal,bhlv->bhav', [att_w, Vx])
        # [B, H, T_q, Dh]
        out = out.permute(0,2,1,3).contiguous()
        # [B, T_q, H, Dh]
        out = out.view(B, -1, self.num_heads * self.head_dim)
        # [B, T_q, Edim]
        out = self.dropout(self.proj_w(out)) 
        # residue dropout

        if use_cache:
            return out, (Kx, Vx)
        
        return out
    
        # Note: Attention weights [T,T] is probability distribution over all tokens (summing to 1), telling how much to borrow from each.
        # Each output feature is a weighted average of the same feature across all tokens’ value vectors.
        # Head divides the feature space vertically.
        # Each head (with its own probability distribution) produces a weighted average of value vectors (features) within its subspace.
        # Here features within single head share a distribution.
        # Intuitively, token vector is updated as if it had a dynamically generated transformation matrix [E,E] applied to it.
        # As GNN: essentially can think of multiple parallel graphs, each head defining its own edge weights and message passing.
        # MHA = each node vertically split, and each slice attends only to corresponding slices in other tokens.

        # KV cache:
        # for each inference the KV cache grow as O(num of layers x seq length x num heads x head dim)

        # To empower kv cache inference, can does it make sense to alter the inductive bias of the model ie maybe multiple q heads share common kv head??

class GPT2MLPF(nn.Module):

    def __init__(self, embed_dim, ff_dim, dropout):
        super().__init__()
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim

        self.emb_ff = nn.Linear(self.embed_dim, self.ff_dim)
        self.ff_emb = nn.Linear(self.ff_dim, self.embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.emb_ff(x)
        x = F.gelu(x, approximate="tanh")
        x = self.ff_emb(x)
        x = self.dropout(x)
        return x
        # Note: The affine transformaiton E -> E is not very expessive. By expanding four times and compressive back learns rich features.
        # This acts like a bottleneck autoencoder
        # In a stack 2/3rd of parameters live in MLPF while rest 1/3rd is in MHA

class GPT2Block(nn.Module):
    def __init__(self, max_length, embed_dim, ff_dim, num_heads, dropout):
        super().__init__()
        self.max_length = max_length
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads

        self.mhattn = GPT2MHAttention(self.max_length, self.embed_dim, self.num_heads, dropout)
        self.mlpf = GPT2MLPF(self.embed_dim, self.ff_dim, dropout)

        self.ln1 = nn.LayerNorm(self.embed_dim)
        self.ln2 = nn.LayerNorm(self.embed_dim)

    def forward(self, x, past_kv=None, use_cache=False):
        if use_cache:
            attd_x, present_kv = self.mhattn(self.ln1(x), past_kv=past_kv, use_cache=True)
            x = x + attd_x
            x = x + self.mlpf(self.ln2(x))
            return x, present_kv
        else:
            x = x + self.mhattn(self.ln1(x))
            x = x + self.mlpf(self.ln2(x))
            return x

class GPT2(nn.Module):

    def __init__(self, vocab_size, max_length, embed_dim, ff_dim, num_heads, num_layers, dropout):
        super().__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.num_layers = num_layers

        self.embedding = GPT2Embedding(self.vocab_size, self.max_length, self.embed_dim, dropout)
        self.lm_head = GPT2LMHead(self.embed_dim, self.embedding) # sending embedding to tie weights

        self.blocks = nn.ModuleList(
            [GPT2Block(max_length, embed_dim, ff_dim, num_heads, dropout) for _ in range(self.num_layers)]
        )

    def forward(self, x, past_key_values=None, use_cache=False):
        B, T = x.size()
        if past_key_values is not None and past_key_values[0] is not None:
            # K: [B, H, T_past, Dh]
            T_past = past_key_values[0][0].size(2)
        else:
            T_past = 0
        assert (T + T_past) <= self.max_length, "sequence length cant exceed max length"

        # [B, SEQ_LENGTH]
        x_pos = torch.arange(T_past, T_past + T, device=x.device).unsqueeze(0)  # [1, T]
        x = self.embedding(x, x_pos=x_pos)
        # [B, SEQ_LENGTH, EMBED_DIM]

        if use_cache:
            if past_key_values is None:
                past_key_values = [None] * self.num_layers
            new_past_key_values = []
            for i, block in enumerate(self.blocks):
                x, present_kv = block(x, past_kv=past_key_values[i], use_cache=True)
                new_past_key_values.append(present_kv)

            x = self.lm_head(x)

            return {
                'logits': x,
                'past_key_values': new_past_key_values
            }

        else:
            for block in self.blocks:
                x = block(x)

            # [B, SEQ_LENGTH, EMBED_DIM]
            x = self.lm_head(x)
            # [B, SEQ_LENGTH, VOCAB_SIZE]

            return {
                'logits': x,
            }
    
    @classmethod
    def from_pretrained(cls, model_type, *, torch_dtype=torch.float32, device="cuda"):

        assert model_type in ('BASE'), "only supports BASE"
        
        config = dict(vocab_size=50257, max_length=1024, embed_dim=768,  ff_dim=768*4,  num_heads=12, num_layers=12, dropout=0.1)
        model = cls(**config).to(dtype=torch_dtype, device=device)

        from utils import download_safetensors, stream_safetensors_to_meta_model

        MODEL_URL = 'https://huggingface.co/openai-community/gpt2/resolve/main/model.safetensors'
        DIR = 'gpt2'

        # 1. model file
        model_file = download_safetensors(MODEL_URL, DIR)

        # 2. meta nn module
        with torch.device('meta'):
            model = cls(**config)

        # 3. embeddings
        embedding_map = {
            'wte.weight':      'embedding.tok_embed.weight',
            'wpe.weight':      'embedding.pos_embed.weight'
        }
        lm_head_map = {
            'ln_f.weight':     'lm_head.ln.weight',
            'ln_f.bias':       'lm_head.ln.bias'
        }
        blocks_map = lambda i: {
            f'h.{i}.ln_1.weight':           f'blocks.{i}.ln1.weight',
            f'h.{i}.ln_1.bias':             f'blocks.{i}.ln1.bias',
            f'h.{i}.attn.c_attn.weight':    f'blocks.{i}.mhattn.qkv_w.weight',
            f'h.{i}.attn.c_attn.bias':      f'blocks.{i}.mhattn.qkv_w.bias',
            f'h.{i}.attn.c_proj.weight':    f'blocks.{i}.mhattn.proj_w.weight',
            f'h.{i}.attn.c_proj.bias':      f'blocks.{i}.mhattn.proj_w.bias',
            f'h.{i}.ln_2.weight':           f'blocks.{i}.ln2.weight',
            f'h.{i}.ln_2.bias':             f'blocks.{i}.ln2.bias',
            f'h.{i}.mlp.c_fc.weight':       f'blocks.{i}.mlpf.emb_ff.weight',
            f'h.{i}.mlp.c_fc.bias':         f'blocks.{i}.mlpf.emb_ff.bias',
            f'h.{i}.mlp.c_proj.weight':     f'blocks.{i}.mlpf.ff_emb.weight',
            f'h.{i}.mlp.c_proj.bias':       f'blocks.{i}.mlpf.ff_emb.bias'
        }
        all_mappings = {**embedding_map, **lm_head_map}
        for i in range(config['num_layers']):
            all_mappings.update(blocks_map(i))

        # 4. needs T if any 
        needs_T = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']

        model = stream_safetensors_to_meta_model(model, model_file, all_mappings, needs_T, torch_dtype, device)

        tokenizer = GPT2BPE.gpt2(pretrained=True)
        return tokenizer, model
    